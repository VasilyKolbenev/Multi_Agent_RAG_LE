from typing import Dict, Any, List, Set, Optional
from .llm import LLM
from .retrieval import HybridCorpus
from .graph_index import GraphIndex
from .langx import run_extraction
import re

SYSTEM_PLANNER = (
    "–¢—ã ‚Äî –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã. –†–∞–∑–±–µ–π –∑–∞–ø—Ä–æ—Å –Ω–∞ —à–∞–≥–∏: retrieval, synthesis, critique. "
    "–í–µ—Ä–Ω–∏ –∫—Ä–∞—Ç–∫–∏–π –ø–ª–∞–Ω (3‚Äì6 –ø—É–Ω–∫—Ç–æ–≤)."
)
SYSTEM_WRITER = (
    "–¢—ã ‚Äî –ü–∏—Å–∞—Ç–µ–ª—å. –ò—Å–ø–æ–ª—å–∑—É–π –≤—ã–¥–µ—Ä–∂–∫–∏ –∏–∑ CONTEXT, —Ü–∏—Ç–∏—Ä—É–π –∏—Å—Ç–æ—á–Ω–∏–∫–∏ [doc_id]. –ù–µ –≤—ã–¥—É–º—ã–≤–∞–π."
)
SYSTEM_CRITIC = (
    "–¢—ã ‚Äî –ö—Ä–∏—Ç–∏–∫. –ü—Ä–æ–≤–µ—Ä—å –æ—Ç–≤–µ—Ç –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É, –¥–∞–π –∑–∞–º–µ—á–∞–Ω–∏—è –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏."
)

async def llm_rerank(query: str, docs: List[Dict[str, Any]], llm: LLM) -> List[Dict[str, Any]]:
    if not docs:
        return []
    prompt = ("–û—Ç—Å–æ—Ä—Ç–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∫ –∑–∞–ø—Ä–æ—Å—É. "
              "–í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ numerated list of indices, for example: 1. 3\n2. 1\n3. 0\n4. 2\n\n"
              f"–ó–∞–ø—Ä–æ—Å: {query}\n\n" +
              "\n".join(f"[{i}] {d['text'][:700]}" for i, d in enumerate(docs)))
    
    try:
        response = await llm.complete("You are a helpful re-ranking assistant.", prompt)
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–∞ –∏–∑ –æ—Ç–≤–µ—Ç–∞
        ranked_indices = [int(i) for i in re.findall(r'\d+', response)]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ –∏–Ω–¥–µ–∫—Å—ã –≤–∞–ª–∏–¥–Ω—ã
        if all(i < len(docs) for i in ranked_indices):
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤
            reranked_docs = [docs[i] for i in ranked_indices]
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –¥–æ–∫—É–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ –±—ã–ª–æ –≤ –æ—Ç–≤–µ—Ç–µ
            original_indices = set(range(len(docs)))
            used_indices = set(ranked_indices)
            remaining_indices = original_indices - used_indices
            for i in remaining_indices:
                reranked_docs.append(docs[i])
            return reranked_docs
        else:
            # –ï—Å–ª–∏ LLM –≤–µ—Ä–Ω—É–ª–∞ –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫
            return docs
    except Exception:
        # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫
        return docs

class MultiAgent:
    def __init__(self, corpus: HybridCorpus, graph: GraphIndex):
        self.corpus = corpus
        self.graph = graph
        self.llm = LLM()

    async def run(self, query: str, k: int = 5, entities_filter: Optional[List[str]] = None, auto_extract: bool = True) -> Dict[str, Any]:
        plan = await self.llm.complete(SYSTEM_PLANNER, query)
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        extracted_entities = []
        if auto_extract and not entities_filter:
            try:
                extraction_result = run_extraction(query, "–ò–∑–≤–ª–µ–∫–∏ –ª—é–¥–µ–π, –∫–æ–º–ø–∞–Ω–∏–∏, –º–µ—Å—Ç–∞, —Å–æ–±—ã—Ç–∏—è –∏ –¥–∞—Ç—ã –∏–∑ —ç—Ç–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞")
                extracted_entities = [item.get("text", "") for item in extraction_result.get("items", []) if item.get("text")]
                entities_filter = extracted_entities[:10]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 10 —Å—É—â–Ω–æ—Å—Ç–µ–π
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π: {e}")
        
        allowed_docs: Optional[Set[str]] = None
        if entities_filter:
            allowed_docs = self.graph.filter_docs(entities_filter)
        
        hits = self.corpus.search(query, k=k, allowed_docs=allowed_docs)
        ctx, cites = "", []
        for doc_id, text, score in hits:
            snippet = text[:1200].replace("\n"," ")
            ctx += f"\n[DOC {doc_id}] {snippet}"
            cites.append(doc_id)
        
        answer = await self.llm.complete(SYSTEM_WRITER, f"Q: {query}\n\nCONTEXT:\n{ctx}")
        critique = await self.llm.complete(SYSTEM_CRITIC, f"–û—Ç–≤–µ—Ç: {answer}\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç: {ctx}")
        
        return {
            "plan": plan, 
            "answer": answer, 
            "critique": critique, 
            "citations": cites,
            "hits": [{"doc_id": d, "score": s} for d,_,s in [(h[0],h[1],h[2]) for h in hits]],
            "extracted_entities": extracted_entities,
            "entities_used": entities_filter or []
        }

    async def stream(self, query: str, k: int = 20, entities_filter: Optional[List[str]] = None, auto_extract: bool = True):
        """–ü–æ—Ç–æ–∫–æ–≤–∞—è –≤–µ—Ä—Å–∏—è RAG-–æ—Ç–≤–µ—Ç–∞."""
        import time
        start_time = time.time()
        print(f"üöÄ RAG Stream started for query: {query[:50]}...")
        
        # –®–∞–≥ 1: –ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ (–Ω–µ –ø–æ—Ç–æ–∫–æ–≤–æ–µ)
        step_start = time.time()
        plan = await self.llm.complete(SYSTEM_PLANNER, query)
        print(f"‚è±Ô∏è Planning took: {time.time() - step_start:.2f}s")
        yield {"type": "plan", "data": plan}
        
        # –®–∞–≥ 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π (–í–†–ï–ú–ï–ù–ù–û –û–¢–ö–õ–Æ–ß–ï–ù–û –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)
        step_start = time.time()
        extracted_entities = []
        # –û–¢–ö–õ–Æ–ß–ï–ù–û: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ –≤–æ–ø—Ä–æ—Å–∞ (—Å–ª–∏—à–∫–æ–º –º–µ–¥–ª–µ–Ω–Ω–æ)
        # if auto_extract and not entities_filter:
        #     try:
        #         extraction_result = run_extraction(query, "–ò–∑–≤–ª–µ–∫–∏ –ª—é–¥–µ–π, –∫–æ–º–ø–∞–Ω–∏–∏, –º–µ—Å—Ç–∞, —Å–æ–±—ã—Ç–∏—è –∏ –¥–∞—Ç—ã –∏–∑ —ç—Ç–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞")
        #         extracted_entities = [item.get("text", "") for item in extraction_result.get("items", []) if item.get("text")]
        #         entities_filter = extracted_entities[:10]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 10 —Å—É—â–Ω–æ—Å—Ç–µ–π
        #     except Exception as e:
        #         print(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π: {e}")
        print(f"‚è±Ô∏è Entity extraction took: {time.time() - step_start:.2f}s (SKIPPED for performance)")
        yield {"type": "entities", "data": extracted_entities}
        
        # –®–∞–≥ 3: –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫
        step_start = time.time()
        hits = self.corpus.search(query, k=k)
        print(f"‚è±Ô∏è Hybrid search took: {time.time() - step_start:.2f}s")
        yield {"type": "search_details", "data": {"search_type": "Hybrid (BM25 + Vector)", "candidates_found": len(hits)}}
        
        # –®–∞–≥ 3.5: LLM Rerank
        step_start = time.time()
        reranked_hits = await llm_rerank(query, hits, self.llm)
        top_hits = reranked_hits[:5] # –ë–µ—Ä–µ–º —Ç–æ–ø-5 –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        print(f"‚è±Ô∏è LLM reranking took: {time.time() - step_start:.2f}s")
        yield {"type": "rerank_details", "data": {"reranker_model": "gpt-4o-mini", "final_context_chunks": len(top_hits)}}

        ctx = ""
        for chunk in top_hits:
            ctx += f"\n[DOC {chunk['doc_id']} Chunk: {chunk['chunk_id']}] {chunk['text']}"
        
        # –°–æ–±–∏—Ä–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ ID —á–∞–Ω–∫–æ–≤ –¥–ª—è —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        chunk_cites = [chunk['chunk_id'] for chunk in top_hits]

        # –ü–µ—Ä–µ–¥–∞–µ–º —Å–∞–º–∏ —á–∞–Ω–∫–∏ –¥–ª—è –º–æ–¥–∞–ª—å–Ω–æ–≥–æ –æ–∫–Ω–∞
        full_hits_for_modal = [{"doc_id": h['chunk_id'], "text_content": h['text']} for h in top_hits]
        yield {"type": "citations", "citations": chunk_cites, "hits": full_hits_for_modal}
        
        # –®–∞–≥ 4: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ (–ø–æ—Ç–æ–∫–æ–≤–æ–µ)
        async for chunk in self.llm.stream(SYSTEM_WRITER, f"Q: {query}\n\nCONTEXT:\n{ctx}"):
            yield {"type": "answer_chunk", "data": chunk}
            
        yield {"type": "answer_done"}
        
        # –®–∞–≥ 5: –ö—Ä–∏—Ç–∏–∫–∞ (–Ω–µ –ø–æ—Ç–æ–∫–æ–≤–æ–µ, –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏)
        print(f"üèÅ Total RAG processing time: {time.time() - start_time:.2f}s")
        # –î–ª—è —ç—Ç–æ–≥–æ –Ω—É–∂–Ω–æ —Å–æ–±—Ä–∞—Ç—å –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç
        full_answer = "" # –≠—Ç–æ –ø–æ—Ç—Ä–µ–±—É–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –ª–æ–≥–∏–∫–µ, –ø–æ–∫–∞ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        # critique = await self.llm.complete(SYSTEM_CRITIC, f"–û—Ç–≤–µ—Ç: {full_answer}\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç: {ctx}")
        # yield {"type": "critique", "data": critique}
