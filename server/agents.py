from typing import Dict, Any, List, Set, Optional
from .llm import LLM
from .retrieval import HybridCorpus
from .graph_index import GraphIndex
from .langx import run_extraction
import re

SYSTEM_PLANNER = (
    "–¢—ã ‚Äî –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã. –†–∞–∑–±–µ–π –∑–∞–ø—Ä–æ—Å –Ω–∞ —à–∞–≥–∏: retrieval, synthesis, critique. "
    "–í–µ—Ä–Ω–∏ –∫—Ä–∞—Ç–∫–∏–π –ø–ª–∞–Ω (3‚Äì6 –ø—É–Ω–∫—Ç–æ–≤)."
)
SYSTEM_WRITER = (
    "–¢—ã ‚Äî –ü–∏—Å–∞—Ç–µ–ª—å. –ò—Å–ø–æ–ª—å–∑—É–π –≤—ã–¥–µ—Ä–∂–∫–∏ –∏–∑ CONTEXT, —Ü–∏—Ç–∏—Ä—É–π –∏—Å—Ç–æ—á–Ω–∏–∫–∏ [doc_id]. –ù–µ –≤—ã–¥—É–º—ã–≤–∞–π."
)
SYSTEM_CRITIC = (
    "–¢—ã ‚Äî –ö—Ä–∏—Ç–∏–∫. –ü—Ä–æ–≤–µ—Ä—å –æ—Ç–≤–µ—Ç –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É, –¥–∞–π –∑–∞–º–µ—á–∞–Ω–∏—è –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏."
)

async def llm_rerank(query: str, docs: List[Dict[str, Any]], llm: LLM) -> List[Dict[str, Any]]:
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π LLM reranking —Å –±–∞—Ç—á–∞–º–∏ –∏ –∫–æ—Ä–æ—Ç–∫–∏–º–∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏.
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –º–∞–∫—Å–∏–º—É–º 10 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –±–∞—Ç—á–∞–º–∏ –ø–æ 5 —Å —Ç–∞–π–º–∞—É—Ç–æ–º 15 —Å–µ–∫.
    """
    if not docs:
        return []
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ —Ç–æ–ø-10 –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
    candidates = docs[:10]
    if len(candidates) <= 3:
        # –ï—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –º–∞–ª–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫—É
        return candidates
    
    try:
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏ –ø–æ 5 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –º–∞–∫—Å–∏–º—É–º
        batch_size = 5
        batches = [candidates[i:i+batch_size] for i in range(0, len(candidates), batch_size)]
        
        ranked_docs = []
        
        for batch_idx, batch in enumerate(batches):
            # –ö–æ—Ä–æ—Ç–∫–∏–π –ø—Ä–æ–º–ø—Ç —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º —Ç–µ–∫—Å—Ç–∞ –¥–æ 200 —Å–∏–º–≤–æ–ª–æ–≤
            batch_prompt = (
                f"Rank these {len(batch)} documents by relevance to query: '{query[:100]}'\n"
                "Return only numbers (most relevant first): 1,2,3...\n\n"
            )
            
            for i, doc in enumerate(batch):
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –¥–æ 200 —Å–∏–º–≤–æ–ª–æ–≤ + –æ—á–∏—Å—Ç–∫–∞
                clean_text = doc['text'][:200].replace('\n', ' ').strip()
                batch_prompt += f"{i+1}. {clean_text}...\n"
            
            # –ö–æ—Ä–æ—Ç–∫–∏–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            system_prompt = "You are a document relevance ranker. Be concise."
            
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–∞–π–º–∞—É—Ç –¥–ª—è –±–∞—Ç—á–∞
                response = await llm.complete(system_prompt, batch_prompt)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–∞ –∏–∑ –æ—Ç–≤–µ—Ç–∞
                ranked_indices = []
                for match in re.findall(r'\b([1-9])\b', response):
                    idx = int(match) - 1  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ 0-based –∏–Ω–¥–µ–∫—Å
                    if 0 <= idx < len(batch):
                        ranked_indices.append(idx)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –ø–æ—Ä—è–¥–∫–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
                for idx in ranked_indices:
                    if idx < len(batch):
                        ranked_docs.append(batch[idx])
                
                # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –±–∞—Ç—á–∞
                used_indices = set(ranked_indices)
                for i, doc in enumerate(batch):
                    if i not in used_indices:
                        ranked_docs.append(doc)
                        
            except Exception as e:
                print(f"‚ö†Ô∏è LLM rerank batch {batch_idx} failed: {e}")
                # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –¥–æ–±–∞–≤–ª—è–µ–º –±–∞—Ç—á –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
                ranked_docs.extend(batch)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –¥–æ–∫—É–º–µ–Ω—Ç—ã (–µ—Å–ª–∏ –±—ã–ª–æ –±–æ–ª—å—à–µ 10)
        if len(docs) > 10:
            ranked_docs.extend(docs[10:])
            
        return ranked_docs
        
    except Exception as e:
        print(f"‚ö†Ô∏è LLM rerank completely failed: {e}")
        # –í —Å–ª—É—á–∞–µ –ø–æ–ª–Ω–æ–π –æ—à–∏–±–∫–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫
        return docs

class MultiAgent:
    def __init__(self, corpus: HybridCorpus, graph: GraphIndex):
        self.corpus = corpus
        self.graph = graph
        self.llm = LLM()

    async def run(self, query: str, k: int = 5, entities_filter: Optional[List[str]] = None, auto_extract: bool = True) -> Dict[str, Any]:
        plan = await self.llm.complete(SYSTEM_PLANNER, query)
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        extracted_entities = []
        if auto_extract and not entities_filter:
            try:
                extraction_result = run_extraction(query, "–ò–∑–≤–ª–µ–∫–∏ –ª—é–¥–µ–π, –∫–æ–º–ø–∞–Ω–∏–∏, –º–µ—Å—Ç–∞, —Å–æ–±—ã—Ç–∏—è –∏ –¥–∞—Ç—ã –∏–∑ —ç—Ç–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞")
                extracted_entities = [item.get("text", "") for item in extraction_result.get("items", []) if item.get("text")]
                entities_filter = extracted_entities[:10]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 10 —Å—É—â–Ω–æ—Å—Ç–µ–π
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π: {e}")
        
        allowed_docs: Optional[Set[str]] = None
        if entities_filter:
            allowed_docs = self.graph.filter_docs(entities_filter)
        
        hits = self.corpus.search(query, k=k, allowed_docs=allowed_docs)
        ctx, cites = "", []
        for doc_id, text, score in hits:
            snippet = text[:1200].replace("\n"," ")
            ctx += f"\n[DOC {doc_id}] {snippet}"
            cites.append(doc_id)
        
        answer = await self.llm.complete(SYSTEM_WRITER, f"Q: {query}\n\nCONTEXT:\n{ctx}")
        critique = await self.llm.complete(SYSTEM_CRITIC, f"–û—Ç–≤–µ—Ç: {answer}\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç: {ctx}")
        
        return {
            "plan": plan, 
            "answer": answer, 
            "critique": critique, 
            "citations": cites,
            "hits": [{"doc_id": d, "score": s} for d,_,s in [(h[0],h[1],h[2]) for h in hits]],
            "extracted_entities": extracted_entities,
            "entities_used": entities_filter or []
        }

    async def stream(self, query: str, k: int = 20, entities_filter: Optional[List[str]] = None, auto_extract: bool = True):
        """–ü–æ—Ç–æ–∫–æ–≤–∞—è –≤–µ—Ä—Å–∏—è RAG-–æ—Ç–≤–µ—Ç–∞."""
        import time
        start_time = time.time()
        print(f"üöÄ RAG Stream started for query: {query[:50]}...")
        
        # –®–∞–≥ 1: –ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ (–Ω–µ –ø–æ—Ç–æ–∫–æ–≤–æ–µ)
        step_start = time.time()
        plan = await self.llm.complete(SYSTEM_PLANNER, query)
        print(f"‚è±Ô∏è Planning took: {time.time() - step_start:.2f}s")
        yield {"type": "plan", "data": plan}
        
        # –®–∞–≥ 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π (–í–†–ï–ú–ï–ù–ù–û –û–¢–ö–õ–Æ–ß–ï–ù–û –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)
        step_start = time.time()
        extracted_entities = []
        # –û–¢–ö–õ–Æ–ß–ï–ù–û: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ –≤–æ–ø—Ä–æ—Å–∞ (—Å–ª–∏—à–∫–æ–º –º–µ–¥–ª–µ–Ω–Ω–æ)
        # if auto_extract and not entities_filter:
        #     try:
        #         extraction_result = run_extraction(query, "–ò–∑–≤–ª–µ–∫–∏ –ª—é–¥–µ–π, –∫–æ–º–ø–∞–Ω–∏–∏, –º–µ—Å—Ç–∞, —Å–æ–±—ã—Ç–∏—è –∏ –¥–∞—Ç—ã –∏–∑ —ç—Ç–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞")
        #         extracted_entities = [item.get("text", "") for item in extraction_result.get("items", []) if item.get("text")]
        #         entities_filter = extracted_entities[:10]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 10 —Å—É—â–Ω–æ—Å—Ç–µ–π
        #     except Exception as e:
        #         print(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π: {e}")
        print(f"‚è±Ô∏è Entity extraction took: {time.time() - step_start:.2f}s (SKIPPED for performance)")
        yield {"type": "entities", "data": extracted_entities}
        
        # –®–∞–≥ 3: –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫
        step_start = time.time()
        hits = self.corpus.search(query, k=k)
        print(f"‚è±Ô∏è Hybrid search took: {time.time() - step_start:.2f}s")
        yield {"type": "search_details", "data": {"search_type": "Hybrid (BM25 + Vector)", "candidates_found": len(hits)}}
        
        # –®–∞–≥ 3.5: LLM Rerank (–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –í–ï–†–°–ò–Ø)
        step_start = time.time()
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π reranking —Å –±–∞—Ç—á–∞–º–∏ –∏ –∫–æ—Ä–æ—Ç–∫–∏–º–∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏
        reranked_hits = await llm_rerank(query, hits, self.llm)
        top_hits = reranked_hits[:5] # –ë–µ—Ä–µ–º —Ç–æ–ø-5 –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        print(f"‚è±Ô∏è LLM reranking took: {time.time() - step_start:.2f}s (OPTIMIZED: batches + short prompts)")
        yield {"type": "rerank_details", "data": {"reranker_model": "gpt-4o-mini (optimized)", "final_context_chunks": len(top_hits)}}

        ctx = ""
        for chunk in top_hits:
            ctx += f"\n[DOC {chunk['doc_id']} Chunk: {chunk['chunk_id']}] {chunk['text']}"
        
        # –°–æ–±–∏—Ä–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ ID —á–∞–Ω–∫–æ–≤ –¥–ª—è —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        chunk_cites = [chunk['chunk_id'] for chunk in top_hits]

        # –ü–µ—Ä–µ–¥–∞–µ–º —Å–∞–º–∏ —á–∞–Ω–∫–∏ –¥–ª—è –º–æ–¥–∞–ª—å–Ω–æ–≥–æ –æ–∫–Ω–∞
        full_hits_for_modal = [{"doc_id": h['chunk_id'], "text_content": h['text']} for h in top_hits]
        yield {"type": "citations", "citations": chunk_cites, "hits": full_hits_for_modal}
        
        # –®–∞–≥ 4: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ (–ø–æ—Ç–æ–∫–æ–≤–æ–µ)
        async for chunk in self.llm.stream(SYSTEM_WRITER, f"Q: {query}\n\nCONTEXT:\n{ctx}"):
            yield {"type": "answer_chunk", "data": chunk}
            
        yield {"type": "answer_done"}
        
        # –®–∞–≥ 5: –ö—Ä–∏—Ç–∏–∫–∞ (–Ω–µ –ø–æ—Ç–æ–∫–æ–≤–æ–µ, –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏)
        print(f"üèÅ Total RAG processing time: {time.time() - start_time:.2f}s")
        # –î–ª—è —ç—Ç–æ–≥–æ –Ω—É–∂–Ω–æ —Å–æ–±—Ä–∞—Ç—å –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç
        full_answer = "" # –≠—Ç–æ –ø–æ—Ç—Ä–µ–±—É–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –ª–æ–≥–∏–∫–µ, –ø–æ–∫–∞ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        # critique = await self.llm.complete(SYSTEM_CRITIC, f"–û—Ç–≤–µ—Ç: {full_answer}\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç: {ctx}")
        # yield {"type": "critique", "data": critique}
